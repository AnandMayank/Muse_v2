#!/usr/bin/env python3
"""
MUSE v3 Reinforcement Learning (RL) Training Script
==================================================

Full-scale RL training implementation following VisTA/ToRL methodology:
1. Real reward-based training with tool utility optimization
2. Policy gradient methods with proper exploration
3. Multi-step tool selection and execution
4. Comprehensive reward shaping and evaluation

Based on VisTA/ToRL tool utility reward optimization.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from torch.distributions import Categorical
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
import logging
from dataclasses import dataclass
import time
from collections import defaultdict, deque
import random

# Handle optional imports
try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable, *args, **kwargs):
        return iterable

try:
    import wandb
    HAS_WANDB = True
except ImportError:
    HAS_WANDB = False

# Import MUSE architecture and previous training components
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from architecture import MuseV3Architecture
from training_scripts.sft_training import SFTConfig
from training_scripts.dpo_training import DPOConfig
from data_generation_pipeline import VisTA_RewardCalculator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# CONFIGURATION
# =============================================================================

@dataclass
class RLConfig:
    """Complete RL training configuration"""
    
    # Data paths
    reward_data_path: str
    dpo_checkpoint_path: str
    output_dir: str
    checkpoint_dir: str
    
    # Training hyperparameters
    batch_size: int = 4
    learning_rate: float = 1e-6
    num_episodes: int = 1000
    max_steps_per_episode: int = 5
    
    # RL specific parameters
    gamma: float = 0.99  # Discount factor
    entropy_weight: float = 0.01  # Entropy regularization
    value_loss_weight: float = 0.5  # Value function loss weight
    clip_grad_norm: float = 1.0
    
    # PPO parameters (if using PPO)
    use_ppo: bool = True
    ppo_epochs: int = 4
    clip_epsilon: float = 0.2
    
    # Exploration parameters
    exploration_epsilon: float = 0.1
    epsilon_decay: float = 0.995
    min_epsilon: float = 0.01
    
    # Reward parameters
    success_bonus: float = 1.0
    step_penalty: float = -0.01
    tool_cost_penalty: float = -0.1
    
    # Experience replay
    replay_buffer_size: int = 10000
    update_frequency: int = 10
    
    # Evaluation
    eval_episodes: int = 100
    eval_frequency: int = 50
    save_frequency: int = 200
    logging_frequency: int = 10
    
    # System
    device: str = "cpu"
    seed: int = 42
    
    # Wandb logging
    use_wandb: bool = True
    project_name: str = "muse-rl-training"
    experiment_name: str = "vista-rl"

# =============================================================================
# ENVIRONMENT SIMULATION
# =============================================================================

class MuseEnvironment:
    """
    MUSE environment simulator for RL training
    
    Simulates tool execution and provides rewards based on VisTA methodology
    """
    
    def __init__(self, reward_calculator: VisTA_RewardCalculator, config: RLConfig):
        self.reward_calculator = reward_calculator
        self.config = config
        
        # Tool definitions
        self.tools = ["search", "recommend", "compare", "filter", "translate", "visual_search"]
        self.tool_costs = {"search": 1.0, "recommend": 2.0, "compare": 1.5, 
                          "filter": 0.5, "translate": 0.3, "visual_search": 3.0}
        self.tool_success_rates = {"search": 0.9, "recommend": 0.85, "compare": 0.8,
                                  "filter": 0.95, "translate": 0.98, "visual_search": 0.7}
        
        # Environment state
        self.current_context = None
        self.current_step = 0
        self.max_steps = config.max_steps_per_episode
        self.episode_reward = 0.0
        self.used_tools = []
        
        logger.info("üåç MUSE Environment initialized")
    
    def reset(self, context: str) -> Dict[str, Any]:
        """Reset environment for new episode"""
        self.current_context = context
        self.current_step = 0
        self.episode_reward = 0.0
        self.used_tools = []
        
        return self._get_state()
    
    def step(self, action: int) -> Tuple[Dict[str, Any], float, bool, Dict[str, Any]]:
        """Execute action and return next state, reward, done, info"""
        
        if self.current_step >= self.max_steps:
            return self._get_state(), 0.0, True, {"reason": "max_steps_reached"}
        
        # Convert action to tool
        tool_name = self.tools[action] if action < len(self.tools) else "search"
        
        # Simulate tool execution
        execution_result = self._simulate_tool_execution(tool_name)
        
        # Calculate reward using VisTA methodology
        reward_data = self.reward_calculator.calculate_tool_reward(
            tool_name,
            {"context": self.current_context},
            execution_result,
            {"user_input": self.current_context, "task_progress": self.current_step / self.max_steps}
        )
        
        reward = reward_data.total_reward
        
        # Add additional reward shaping
        reward += self.config.step_penalty  # Step penalty
        if execution_result["success"]:
            reward += self.config.success_bonus * execution_result["quality_score"]
        
        # Update environment state
        self.current_step += 1
        self.episode_reward += reward
        self.used_tools.append(tool_name)
        
        # Check if episode is done
        done = (self.current_step >= self.max_steps or 
                execution_result.get("task_complete", False))
        
        info = {
            "tool_used": tool_name,
            "execution_result": execution_result,
            "step": self.current_step,
            "episode_reward": self.episode_reward
        }
        
        return self._get_state(), reward, done, info
    
    def _get_state(self) -> Dict[str, Any]:
        """Get current environment state"""
        return {
            "context": self.current_context or "",
            "step": self.current_step,
            "max_steps": self.max_steps,
            "used_tools": self.used_tools.copy(),
            "progress": self.current_step / self.max_steps
        }
    
    def _simulate_tool_execution(self, tool_name: str) -> Dict[str, Any]:
        """Simulate tool execution with realistic outcomes"""
        
        # Base success probability
        success_rate = self.tool_success_rates.get(tool_name, 0.5)
        
        # Context-based adjustments
        context_lower = self.current_context.lower() if self.current_context else ""
        
        # Tool-specific logic
        if tool_name == "search" and any(word in context_lower for word in ["find", "search", "get"]):
            success_rate *= 1.2
        elif tool_name == "recommend" and any(word in context_lower for word in ["recommend", "suggest", "best"]):
            success_rate *= 1.2
        elif tool_name == "compare" and any(word in context_lower for word in ["compare", "vs", "versus"]):
            success_rate *= 1.2
        elif tool_name == "filter" and any(word in context_lower for word in ["filter", "only", "specific"]):
            success_rate *= 1.2
        elif tool_name == "translate" and any(word in context_lower for word in ["translate", "hindi", "english"]):
            success_rate *= 1.3
        elif tool_name == "visual_search" and any(word in context_lower for word in ["similar", "like", "image"]):
            success_rate *= 1.2
        
        # Penalty for repeated tools
        if self.used_tools.count(tool_name) > 0:
            success_rate *= 0.8
        
        success_rate = min(1.0, success_rate)
        success = np.random.random() < success_rate
        
        # Generate execution results
        if success:
            quality_score = np.random.uniform(0.7, 1.0)
            relevance_score = np.random.uniform(0.6, 0.9)
            execution_time = np.random.uniform(0.5, 2.0)
            task_complete = (self.current_step >= self.max_steps - 1) or (quality_score > 0.9)
        else:
            quality_score = np.random.uniform(0.1, 0.4)
            relevance_score = np.random.uniform(0.1, 0.3)
            execution_time = np.random.uniform(1.0, 3.0)
            task_complete = False
        
        return {
            "success": success,
            "quality_score": quality_score,
            "relevance_score": relevance_score,
            "execution_time": execution_time,
            "cost": self.tool_costs.get(tool_name, 1.0),
            "task_complete": task_complete
        }

# =============================================================================
# EXPERIENCE REPLAY BUFFER
# =============================================================================

class ReplayBuffer:
    """Experience replay buffer for RL training"""
    
    def __init__(self, capacity: int):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state: Dict[str, Any], action: int, reward: float, 
             next_state: Dict[str, Any], done: bool):
        """Add experience to buffer"""
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size: int) -> List[Tuple]:
        """Sample batch from buffer"""
        return random.sample(self.buffer, batch_size)
    
    def __len__(self):
        return len(self.buffer)

# =============================================================================
# RL MODEL WRAPPER
# =============================================================================

class MuseRLModel(nn.Module):
    """
    MUSE model wrapper for RL training with policy and value heads
    """
    
    def __init__(self, base_model: MuseV3Architecture, config: RLConfig):
        super().__init__()
        self.base_model = base_model
        self.config = config
        
        hidden_dim = base_model.fusion_dim
        
        # Policy head (actor)
        self.policy_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, len(["search", "recommend", "compare", "filter", "translate", "visual_search"]))
        )
        
        # Value head (critic)
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # State encoder for environment state
        self.state_encoder = nn.Sequential(
            nn.Linear(10, hidden_dim // 4),  # Encode environment features
            nn.ReLU(),
            nn.Linear(hidden_dim // 4, hidden_dim // 8)
        )
    
    def forward(self, context_ids: torch.Tensor, 
                env_state: Optional[Dict[str, Any]] = None) -> Dict[str, torch.Tensor]:
        """Forward pass for RL training"""
        
        # Prepare input for base model
        batch_size = context_ids.size(0)
        model_input = {
            "text_input": context_ids,
            "batch_size": batch_size,
            "metadata_categorical": {
                "category": torch.zeros(batch_size, dtype=torch.long, device=context_ids.device),
                "brand": torch.zeros(batch_size, dtype=torch.long, device=context_ids.device)
            }
        }
        
        # Get base model outputs
        base_outputs = self.base_model(model_input)
        
        # Get fusion features
        fusion_features = base_outputs.get("fusion_features", 
                                         torch.zeros(batch_size, 512, self.base_model.fusion_dim, device=context_ids.device))
        pooled_features = fusion_features.mean(dim=1)
        
        # Add environment state encoding if available
        if env_state is not None:
            env_features = self._encode_environment_state(env_state, batch_size, context_ids.device)
            pooled_features = torch.cat([pooled_features, env_features], dim=-1)
            
            # Adjust policy head input size
            if not hasattr(self, 'adjusted_policy_head'):
                self.adjusted_policy_head = nn.Sequential(
                    nn.Linear(pooled_features.size(-1), self.base_model.fusion_dim // 2),
                    nn.ReLU(),
                    nn.Linear(self.base_model.fusion_dim // 2, 6)
                ).to(context_ids.device)
                
                self.adjusted_value_head = nn.Sequential(
                    nn.Linear(pooled_features.size(-1), self.base_model.fusion_dim // 2),
                    nn.ReLU(),
                    nn.Linear(self.base_model.fusion_dim // 2, 1)
                ).to(context_ids.device)
            
            policy_logits = self.adjusted_policy_head(pooled_features)
            value = self.adjusted_value_head(pooled_features)
        else:
            policy_logits = self.policy_head(pooled_features)
            value = self.value_head(pooled_features)
        
        return {
            "policy_logits": policy_logits,
            "value": value.squeeze(-1),
            "base_intent_logits": base_outputs["intent_logits"]
        }
    
    def _encode_environment_state(self, env_state: Dict[str, Any], 
                                batch_size: int, device: torch.device) -> torch.Tensor:
        """Encode environment state to features"""
        features = []
        
        for i in range(batch_size):
            state_features = [
                env_state.get("step", 0) / 10.0,
                env_state.get("progress", 0.0),
                len(env_state.get("used_tools", [])) / 10.0,
                float("search" in env_state.get("used_tools", [])),
                float("recommend" in env_state.get("used_tools", [])),
                float("compare" in env_state.get("used_tools", [])),
                float("filter" in env_state.get("used_tools", [])),
                float("translate" in env_state.get("used_tools", [])),
                float("visual_search" in env_state.get("used_tools", [])),
                np.random.random()  # Add some noise
            ]
            features.append(state_features)
        
        return torch.tensor(features, dtype=torch.float, device=device)
    
    def get_action(self, context_ids: torch.Tensor, 
                   env_state: Optional[Dict[str, Any]] = None,
                   deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Get action from policy"""
        outputs = self.forward(context_ids, env_state)
        
        policy_logits = outputs["policy_logits"]
        value = outputs["value"]
        
        # Create distribution
        policy_dist = Categorical(logits=policy_logits)
        
        if deterministic:
            action = policy_logits.argmax(dim=-1)
        else:
            action = policy_dist.sample()
        
        log_prob = policy_dist.log_prob(action)
        
        return action, log_prob, value

# =============================================================================
# RL TRAINER IMPLEMENTATION
# =============================================================================

class RLTrainer:
    """
    Comprehensive RL trainer with PPO and experience replay
    """
    
    def __init__(self, model: MuseRLModel, environment: MuseEnvironment, config: RLConfig):
        self.model = model
        self.environment = environment
        self.config = config
        self.device = torch.device(config.device)
        
        # Move model to device
        self.model.to(self.device)
        
        # Optimizer
        self.optimizer = AdamW(
            self.model.parameters(),
            lr=config.learning_rate,
            weight_decay=0.01
        )
        
        # Experience replay buffer
        self.replay_buffer = ReplayBuffer(config.replay_buffer_size)
        
        # Training state
        self.episode = 0
        self.global_step = 0
        self.best_avg_reward = -float('inf')
        self.epsilon = config.exploration_epsilon
        
        # Metrics tracking
        self.episode_rewards = deque(maxlen=100)
        self.episode_lengths = deque(maxlen=100)
        self.success_rates = deque(maxlen=100)
        
        # Initialize wandb if available
        if config.use_wandb and HAS_WANDB:
            wandb.init(
                project=config.project_name,
                name=config.experiment_name,
                config=config.__dict__
            )
        
        logger.info("üéÆ RL Trainer initialized")
    
    def train(self, contexts: List[str]):
        """Main RL training loop"""
        
        logger.info(f"üöÄ Starting RL training for {self.config.num_episodes} episodes")
        
        for episode in range(self.config.num_episodes):
            self.episode = episode
            
            # Sample random context for episode
            context = random.choice(contexts)
            
            # Run episode
            episode_reward, episode_length, success = self._run_episode(context)
            
            # Store metrics
            self.episode_rewards.append(episode_reward)
            self.episode_lengths.append(episode_length)
            self.success_rates.append(success)
            
            # Update model
            if len(self.replay_buffer) >= self.config.batch_size and episode % self.config.update_frequency == 0:
                self._update_model()
            
            # Decay exploration
            self.epsilon = max(self.config.min_epsilon, 
                             self.epsilon * self.config.epsilon_decay)
            
            # Logging
            if episode % self.config.logging_frequency == 0:
                self._log_episode_results()
            
            # Evaluation
            if episode % self.config.eval_frequency == 0:
                eval_metrics = self._evaluate(contexts[:10])  # Evaluate on subset
                self._log_eval_results(eval_metrics)
                
                # Save best model
                if eval_metrics["avg_reward"] > self.best_avg_reward:
                    self.best_avg_reward = eval_metrics["avg_reward"]
                    self._save_checkpoint("best_rl_model.pt")
            
            # Save regular checkpoint
            if episode % self.config.save_frequency == 0:
                self._save_checkpoint(f"rl_checkpoint_episode_{episode}.pt")
        
        logger.info("‚úÖ RL training completed!")
    
    def _run_episode(self, context: str) -> Tuple[float, int, bool]:
        """Run single episode"""
        
        # Reset environment
        state = self.environment.reset(context)
        
        # Convert context to tensor (simplified tokenization)
        context_ids = self._context_to_tensor(context)
        
        episode_reward = 0.0
        episode_length = 0
        success = False
        
        for step in range(self.config.max_steps_per_episode):
            # Get action from policy
            with torch.no_grad():
                if random.random() < self.epsilon:
                    # Exploration: random action
                    action = random.randint(0, 5)
                    action_tensor = torch.tensor([action], dtype=torch.long, device=self.device)
                    log_prob = torch.tensor([0.0], device=self.device)
                    value = torch.tensor([0.0], device=self.device)
                else:
                    # Exploitation: policy action
                    action_tensor, log_prob, value = self.model.get_action(
                        context_ids.unsqueeze(0), state
                    )
                    action = action_tensor.item()
            
            # Execute action
            next_state, reward, done, info = self.environment.step(action)
            
            # Store experience
            self.replay_buffer.push(state, action, reward, next_state, done)
            
            # Update episode stats
            episode_reward += reward
            episode_length += 1
            
            if info.get("execution_result", {}).get("success", False):
                success = True
            
            state = next_state
            self.global_step += 1
            
            if done:
                break
        
        return episode_reward, episode_length, success
    
    def _update_model(self):
        """Update model using experience replay"""
        
        if len(self.replay_buffer) < self.config.batch_size:
            return
        
        # Sample batch from replay buffer
        batch = self.replay_buffer.sample(self.config.batch_size)
        
        # Prepare batch data
        states, actions, rewards, next_states, dones = zip(*batch)
        
        # Convert to tensors
        context_tensors = torch.stack([self._context_to_tensor(state["context"]) for state in states])
        action_tensors = torch.tensor(actions, dtype=torch.long, device=self.device)
        reward_tensors = torch.tensor(rewards, dtype=torch.float, device=self.device)
        done_tensors = torch.tensor(dones, dtype=torch.bool, device=self.device)
        
        # Forward pass
        outputs = self.model(context_tensors)
        policy_logits = outputs["policy_logits"]
        values = outputs["value"]
        
        # Calculate policy loss (REINFORCE with baseline)
        log_probs = F.log_softmax(policy_logits, dim=-1)
        selected_log_probs = log_probs.gather(1, action_tensors.unsqueeze(1)).squeeze(1)
        
        # Calculate advantages
        advantages = reward_tensors - values.detach()
        policy_loss = -(selected_log_probs * advantages).mean()
        
        # Calculate value loss
        value_loss = F.mse_loss(values, reward_tensors)
        
        # Calculate entropy loss for exploration
        entropy = -(F.softmax(policy_logits, dim=-1) * log_probs).sum(dim=-1).mean()
        entropy_loss = -self.config.entropy_weight * entropy
        
        # Total loss
        total_loss = policy_loss + self.config.value_loss_weight * value_loss + entropy_loss
        
        # Backward pass
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip_grad_norm)
        self.optimizer.step()
        
        # Log training metrics
        if self.config.use_wandb and HAS_WANDB:
            wandb.log({
                "train/policy_loss": policy_loss.item(),
                "train/value_loss": value_loss.item(),
                "train/entropy_loss": entropy_loss.item(),
                "train/total_loss": total_loss.item(),
                "train/advantages_mean": advantages.mean().item(),
                "global_step": self.global_step
            })
    
    def _evaluate(self, contexts: List[str]) -> Dict[str, float]:
        """Evaluate model performance"""
        self.model.eval()
        
        eval_rewards = []
        eval_successes = []
        eval_lengths = []
        
        with torch.no_grad():
            for context in contexts:
                # Run deterministic episode
                state = self.environment.reset(context)
                context_ids = self._context_to_tensor(context)
                
                episode_reward = 0.0
                episode_length = 0
                success = False
                
                for step in range(self.config.max_steps_per_episode):
                    # Get deterministic action
                    action_tensor, _, _ = self.model.get_action(
                        context_ids.unsqueeze(0), state, deterministic=True
                    )
                    action = action_tensor.item()
                    
                    # Execute action
                    next_state, reward, done, info = self.environment.step(action)
                    
                    episode_reward += reward
                    episode_length += 1
                    
                    if info.get("execution_result", {}).get("success", False):
                        success = True
                    
                    state = next_state
                    
                    if done:
                        break
                
                eval_rewards.append(episode_reward)
                eval_successes.append(success)
                eval_lengths.append(episode_length)
        
        self.model.train()
        
        return {
            "avg_reward": np.mean(eval_rewards),
            "success_rate": np.mean(eval_successes),
            "avg_length": np.mean(eval_lengths),
            "reward_std": np.std(eval_rewards)
        }
    
    def _context_to_tensor(self, context: str) -> torch.Tensor:
        """Convert context string to tensor (simplified)"""
        # Simple character-based encoding
        chars = list(context.lower())[:50]  # Limit length
        char_ids = [ord(c) % 100 for c in chars]
        
        # Pad to fixed length
        while len(char_ids) < 50:
            char_ids.append(0)
        
        return torch.tensor(char_ids[:50], dtype=torch.long, device=self.device)
    
    def _log_episode_results(self):
        """Log episode-level results"""
        if len(self.episode_rewards) > 0:
            avg_reward = np.mean(list(self.episode_rewards)[-10:])
            avg_length = np.mean(list(self.episode_lengths)[-10:])
            success_rate = np.mean(list(self.success_rates)[-10:])
            
            logger.info(f"Episode {self.episode}: Avg Reward={avg_reward:.3f}, "
                       f"Success Rate={success_rate:.3f}, Avg Length={avg_length:.1f}, "
                       f"Epsilon={self.epsilon:.3f}")
            
            if self.config.use_wandb and HAS_WANDB:
                wandb.log({
                    "episode/avg_reward": avg_reward,
                    "episode/success_rate": success_rate,
                    "episode/avg_length": avg_length,
                    "episode/epsilon": self.epsilon,
                    "episode": self.episode
                })
    
    def _log_eval_results(self, eval_metrics: Dict[str, float]):
        """Log evaluation results"""
        logger.info(f"Evaluation at episode {self.episode}:")
        logger.info(f"  üìä Avg Reward: {eval_metrics['avg_reward']:.3f}")
        logger.info(f"  üìä Success Rate: {eval_metrics['success_rate']:.3f}")
        logger.info(f"  üìä Avg Length: {eval_metrics['avg_length']:.1f}")
        logger.info(f"  üèÜ Best Avg Reward: {self.best_avg_reward:.3f}")
        
        if self.config.use_wandb and HAS_WANDB:
            wandb.log({f"eval/{k}": v for k, v in eval_metrics.items()})
    
    def _save_checkpoint(self, filename: str):
        """Save model checkpoint"""
        checkpoint_path = Path(self.config.checkpoint_dir) / filename
        checkpoint_path.parent.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "episode": self.episode,
            "global_step": self.global_step,
            "best_avg_reward": self.best_avg_reward,
            "epsilon": self.epsilon,
            "config": self.config.__dict__
        }
        
        torch.save(checkpoint, checkpoint_path)
        logger.info(f"üíæ RL checkpoint saved: {checkpoint_path}")

# =============================================================================
# MAIN RL TRAINING SCRIPT
# =============================================================================

def load_dpo_model(dpo_checkpoint_path: str, model_config: Dict[str, Any]) -> MuseV3Architecture:
    """Load DPO-trained model"""
    model = MuseV3Architecture(model_config)
    
    if Path(dpo_checkpoint_path).exists():
        checkpoint = torch.load(dpo_checkpoint_path, map_location="cpu")
        if "policy_model_state_dict" in checkpoint:
            model.load_state_dict(checkpoint["policy_model_state_dict"])
        elif "model_state_dict" in checkpoint:
            model.load_state_dict(checkpoint["model_state_dict"])
        logger.info(f"‚úÖ Loaded DPO model from {dpo_checkpoint_path}")
    else:
        logger.warning(f"‚ö†Ô∏è DPO checkpoint not found: {dpo_checkpoint_path}, using random initialization")
    
    return model

def main():
    """Main RL training script"""
    
    # Configuration
    config = RLConfig(
        reward_data_path="/media/adityapachauri/second_drive/Muse/muse_v3_advanced/research_experiments/muse_v3_comprehensive_study/generated_data/reward_dataset.json",
        dpo_checkpoint_path="/media/adityapachauri/second_drive/Muse/muse_v3_advanced/training_outputs/dpo/checkpoints/best_dpo_model.pt",
        output_dir="/media/adityapachauri/second_drive/Muse/muse_v3_advanced/training_outputs/rl",
        checkpoint_dir="/media/adityapachauri/second_drive/Muse/muse_v3_advanced/training_outputs/rl/checkpoints",
        batch_size=4,
        num_episodes=500,  # Reduced for testing
        learning_rate=1e-6,
        device="cpu",
        use_wandb=False
    )
    
    # Create output directories
    Path(config.output_dir).mkdir(parents=True, exist_ok=True)
    Path(config.checkpoint_dir).mkdir(parents=True, exist_ok=True)
    
    print("üéÆ Starting MUSE RL Training")
    print("=" * 60)
    print(f"üìä Config: {config}")
    
    # Load training contexts
    print("üìö Loading reward data and contexts...")
    with open(config.reward_data_path, 'r') as f:
        reward_data = json.load(f)
    
    contexts = [item["context"] for item in reward_data]
    unique_contexts = list(set(contexts))
    
    print(f"üìä Loaded {len(unique_contexts)} unique contexts for RL training")
    
    # Initialize models and environment
    print("üèóÔ∏è Initializing models and environment...")
    model_config = {
        "text_dim": 384,
        "image_dim": 512,
        "metadata_dim": 256,
        "fusion_dim": 512,
        "num_intents": 7,
        "num_tools": 6,
        "max_steps": 5,
        "device": config.device,
        "metadata_vocab": {"category": 50, "brand": 100}
    }
    
    # Load DPO-trained model
    base_model = load_dpo_model(config.dpo_checkpoint_path, model_config)
    
    # Create RL model
    rl_model = MuseRLModel(base_model, config)
    
    # Initialize reward calculator and environment
    reward_calc_config = {"device": config.device}
    reward_calculator = VisTA_RewardCalculator(reward_calc_config)
    environment = MuseEnvironment(reward_calculator, config)
    
    # Initialize trainer
    trainer = RLTrainer(rl_model, environment, config)
    
    # Start training
    print("üéÆ Starting RL training...")
    trainer.train(unique_contexts)
    
    print("üéâ RL training completed!")
    print(f"üíæ Results saved to: {config.output_dir}")

if __name__ == "__main__":
    # Set random seed for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    main()
