#!/usr/bin/env python3
"""
Enhanced DPO Data Pipeline for MUSE v3
======================================

Comprehensive data processing pipeline for DPO training with:
1. Advanced preference pair validation and processing
2. Tool-aware trajectory analysis
3. Multi-format data loading (JSON, JSONL, CSV)
4. Quality filtering and augmentation
5. Batch processing optimization
6. Real-time data statistics and monitoring

Based on DiaTool methodology and VisTA reward modeling.
"""

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Sampler
import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional, Union
import logging
from dataclasses import dataclass, asdict
import time
from collections import defaultdict, Counter
import copy
import random
import math
import re
import os
import sys

# Add parent directory for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# =============================================================================
# DATA STRUCTURES
# =============================================================================

@dataclass
class TrajectoryStep:
    """Single step in a trajectory"""
    tool_name: str
    arguments: Dict[str, Any]
    position: int
    confidence: float = 1.0
    execution_time: float = 0.0
    success: bool = True

@dataclass
class ProcessedTrajectory:
    """Processed trajectory with metadata"""
    steps: List[TrajectoryStep]
    total_length: int
    tool_diversity: float
    efficiency_score: float
    success_rate: float
    metadata: Dict[str, Any]

@dataclass
class EnhancedPreferencePair:
    """Enhanced preference pair with comprehensive metadata"""
    context: str
    chosen_trajectory: ProcessedTrajectory
    rejected_trajectory: ProcessedTrajectory
    preference_score: float
    preference_strength: float
    quality_metrics: Dict[str, float]
    metadata: Dict[str, Any]

# =============================================================================
# ADVANCED DATA PROCESSOR
# =============================================================================

class AdvancedDPODataProcessor:
    """
    Advanced processor for DPO preference pairs
    
    Features:
    - Multi-format data loading
    - Trajectory quality analysis
    - Tool usage pattern recognition
    - Preference strength calculation
    - Quality filtering and validation
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.tool_vocabulary = self._build_tool_vocabulary()
        self.quality_thresholds = config.get("quality_thresholds", {
            "min_preference_strength": 0.1,
            "min_trajectory_length": 1,
            "max_trajectory_length": 10,
            "min_tool_diversity": 0.0
        })
        
        # Statistics tracking
        self.processing_stats = defaultdict(int)
        self.quality_stats = defaultdict(list)
        
        logger.info("ðŸ”§ Advanced DPO Data Processor initialized")
    
    def _build_tool_vocabulary(self) -> Dict[str, int]:
        """Build vocabulary of available tools"""
        tools = [
            "search", "recommend", "compare", "visual_search", 
            "translate", "analyze", "filter", "sort", "unknown"
        ]
        return {tool: idx for idx, tool in enumerate(tools)}
    
    def load_and_process_data(self, data_path: str) -> List[EnhancedPreferencePair]:
        """Load and process DPO data from various formats"""
        data_path = Path(data_path)
        
        if not data_path.exists():
            raise FileNotFoundError(f"Data file not found: {data_path}")
        
        # Load raw data based on format
        if data_path.suffix == '.json':
            raw_data = self._load_json(data_path)
        elif data_path.suffix == '.jsonl':
            raw_data = self._load_jsonl(data_path)
        elif data_path.suffix == '.csv':
            raw_data = self._load_csv(data_path)
        else:
            raise ValueError(f"Unsupported file format: {data_path.suffix}")
        
        logger.info(f"ðŸ“š Loaded {len(raw_data)} raw preference pairs")
        
        # Process and enhance data
        processed_pairs = []
        for idx, item in enumerate(raw_data):
            try:
                enhanced_pair = self._process_preference_pair(item, idx)
                if enhanced_pair and self._validate_pair(enhanced_pair):
                    processed_pairs.append(enhanced_pair)
                    self.processing_stats["valid_pairs"] += 1
                else:
                    self.processing_stats["invalid_pairs"] += 1
            except Exception as e:
                logger.warning(f"Failed to process pair {idx}: {e}")
                self.processing_stats["processing_errors"] += 1
        
        logger.info(f"âœ… Processed {len(processed_pairs)} valid preference pairs")
        self._log_processing_statistics()
        
        return processed_pairs
    
    def _load_json(self, path: Path) -> List[Dict[str, Any]]:
        """Load JSON format data"""
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _load_jsonl(self, path: Path) -> List[Dict[str, Any]]:
        """Load JSONL format data"""
        data = []
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line.strip()))
        return data
    
    def _load_csv(self, path: Path) -> List[Dict[str, Any]]:
        """Load CSV format data"""
        df = pd.read_csv(path)
        return df.to_dict('records')
    
    def _process_preference_pair(self, item: Dict[str, Any], idx: int) -> Optional[EnhancedPreferencePair]:
        """Process individual preference pair with enhancements"""
        try:
            # Extract basic information
            context = item.get("context", "")
            if not context:
                return None
            
            # Process trajectories
            chosen_trajectory = self._process_trajectory(
                item.get("chosen_trajectory", []), "chosen"
            )
            rejected_trajectory = self._process_trajectory(
                item.get("rejected_trajectory", []), "rejected"
            )
            
            if not chosen_trajectory or not rejected_trajectory:
                return None
            
            # Calculate preference metrics
            preference_score = float(item.get("preference_score", 0.5))
            preference_strength = self._calculate_preference_strength(
                chosen_trajectory, rejected_trajectory, preference_score
            )
            
            # Calculate quality metrics
            quality_metrics = self._calculate_quality_metrics(
                chosen_trajectory, rejected_trajectory, context
            )
            
            # Enhanced metadata
            metadata = {
                "original_metadata": item.get("metadata", {}),
                "pair_id": idx,
                "processing_timestamp": time.time(),
                "context_length": len(context.split()),
                "chosen_tools": [step.tool_name for step in chosen_trajectory.steps],
                "rejected_tools": [step.tool_name for step in rejected_trajectory.steps]
            }
            
            return EnhancedPreferencePair(
                context=context,
                chosen_trajectory=chosen_trajectory,
                rejected_trajectory=rejected_trajectory,
                preference_score=preference_score,
                preference_strength=preference_strength,
                quality_metrics=quality_metrics,
                metadata=metadata
            )
            
        except Exception as e:
            logger.warning(f"Error processing preference pair {idx}: {e}")
            return None
    
    def _process_trajectory(self, trajectory_data: List[Dict[str, Any]], 
                          trajectory_type: str) -> Optional[ProcessedTrajectory]:
        """Process trajectory with advanced analysis"""
        if not trajectory_data:
            return None
        
        steps = []
        tool_counts = Counter()
        
        for step_data in trajectory_data:
            try:
                tool_call = step_data.get("tool_call", {})
                tool_name = tool_call.get("tool_name", "unknown")
                arguments = tool_call.get("arguments", {})
                position = step_data.get("position", 0)
                
                # Create trajectory step
                step = TrajectoryStep(
                    tool_name=tool_name,
                    arguments=arguments,
                    position=position,
                    confidence=self._calculate_step_confidence(tool_name, arguments),
                    success=True  # Assume success for now
                )
                
                steps.append(step)
                tool_counts[tool_name] += 1
                
            except Exception as e:
                logger.warning(f"Error processing trajectory step: {e}")
                continue
        
        if not steps:
            return None
        
        # Calculate trajectory metrics
        total_length = len(steps)
        tool_diversity = len(tool_counts) / total_length if total_length > 0 else 0.0
        efficiency_score = self._calculate_efficiency_score(steps)
        success_rate = sum(1 for step in steps if step.success) / total_length
        
        return ProcessedTrajectory(
            steps=steps,
            total_length=total_length,
            tool_diversity=tool_diversity,
            efficiency_score=efficiency_score,
            success_rate=success_rate,
            metadata={
                "trajectory_type": trajectory_type,
                "tool_distribution": dict(tool_counts),
                "avg_position": np.mean([step.position for step in steps])
            }
        )
    
    def _calculate_step_confidence(self, tool_name: str, arguments: Dict[str, Any]) -> float:
        """Calculate confidence score for a trajectory step"""
        base_confidence = 0.8
        
        # Boost confidence for known tools
        if tool_name in self.tool_vocabulary:
            base_confidence += 0.1
        
        # Boost confidence for well-formed arguments
        if arguments and len(arguments) > 0:
            base_confidence += 0.1
        
        return min(base_confidence, 1.0)
    
    def _calculate_efficiency_score(self, steps: List[TrajectoryStep]) -> float:
        """Calculate efficiency score for trajectory"""
        if not steps:
            return 0.0
        
        # Simple efficiency metric based on tool diversity and step count
        unique_tools = len(set(step.tool_name for step in steps))
        total_steps = len(steps)
        
        # Prefer trajectories with good tool diversity but not too many steps
        diversity_score = unique_tools / total_steps
        length_penalty = max(0, 1.0 - (total_steps - 3) * 0.1)  # Penalty for >3 steps
        
        return diversity_score * length_penalty
    
    def _calculate_preference_strength(self, chosen: ProcessedTrajectory, 
                                     rejected: ProcessedTrajectory, 
                                     preference_score: float) -> float:
        """Calculate preference strength based on trajectory differences"""
        # Base strength from preference score
        base_strength = abs(preference_score - 0.5) * 2
        
        # Enhance strength based on trajectory differences
        efficiency_diff = abs(chosen.efficiency_score - rejected.efficiency_score)
        diversity_diff = abs(chosen.tool_diversity - rejected.tool_diversity)
        length_diff = abs(chosen.total_length - rejected.total_length) / 10.0
        
        # Combine factors
        enhanced_strength = base_strength + 0.3 * efficiency_diff + 0.2 * diversity_diff + 0.1 * length_diff
        
        return min(enhanced_strength, 1.0)
    
    def _calculate_quality_metrics(self, chosen: ProcessedTrajectory, 
                                 rejected: ProcessedTrajectory, 
                                 context: str) -> Dict[str, float]:
        """Calculate comprehensive quality metrics"""
        return {
            "trajectory_length_ratio": chosen.total_length / max(rejected.total_length, 1),
            "efficiency_advantage": chosen.efficiency_score - rejected.efficiency_score,
            "diversity_advantage": chosen.tool_diversity - rejected.tool_diversity,
            "success_rate_advantage": chosen.success_rate - rejected.success_rate,
            "context_relevance": self._calculate_context_relevance(chosen, context),
            "tool_appropriateness": self._calculate_tool_appropriateness(chosen, context)
        }
    
    def _calculate_context_relevance(self, trajectory: ProcessedTrajectory, context: str) -> float:
        """Calculate how relevant trajectory is to context"""
        # Simple keyword matching approach
        context_words = set(context.lower().split())
        
        relevance_score = 0.0
        for step in trajectory.steps:
            # Check if tool name relates to context
            if any(word in step.tool_name.lower() for word in context_words):
                relevance_score += 0.2
            
            # Check arguments for relevance
            for arg_value in step.arguments.values():
                if isinstance(arg_value, str):
                    arg_words = set(arg_value.lower().split())
                    overlap = len(context_words.intersection(arg_words))
                    relevance_score += overlap * 0.1
        
        return min(relevance_score / len(trajectory.steps), 1.0)
    
    def _calculate_tool_appropriateness(self, trajectory: ProcessedTrajectory, context: str) -> float:
        """Calculate tool appropriateness for context"""
        # Simple heuristic based on context keywords
        context_lower = context.lower()
        
        appropriateness_score = 0.0
        for step in trajectory.steps:
            tool = step.tool_name.lower()
            
            # Context-tool matching heuristics
            if "search" in context_lower and tool == "search":
                appropriateness_score += 0.3
            elif "recommend" in context_lower and tool == "recommend":
                appropriateness_score += 0.3
            elif "compare" in context_lower and tool == "compare":
                appropriateness_score += 0.3
            elif "visual" in context_lower and tool == "visual_search":
                appropriateness_score += 0.3
            else:
                appropriateness_score += 0.1  # Base score for any tool use
        
        return min(appropriateness_score / len(trajectory.steps), 1.0)
    
    def _validate_pair(self, pair: EnhancedPreferencePair) -> bool:
        """Validate preference pair against quality thresholds"""
        thresholds = self.quality_thresholds
        
        # Check preference strength
        if pair.preference_strength < thresholds["min_preference_strength"]:
            return False
        
        # Check trajectory lengths
        chosen_len = pair.chosen_trajectory.total_length
        rejected_len = pair.rejected_trajectory.total_length
        
        if (chosen_len < thresholds["min_trajectory_length"] or 
            chosen_len > thresholds["max_trajectory_length"] or
            rejected_len < thresholds["min_trajectory_length"] or
            rejected_len > thresholds["max_trajectory_length"]):
            return False
        
        # Check tool diversity
        if (pair.chosen_trajectory.tool_diversity < thresholds["min_tool_diversity"] or
            pair.rejected_trajectory.tool_diversity < thresholds["min_tool_diversity"]):
            return False
        
        return True
    
    def _log_processing_statistics(self):
        """Log processing statistics"""
        logger.info("ðŸ“Š Data Processing Statistics:")
        for key, value in self.processing_stats.items():
            logger.info(f"  - {key}: {value}")
        
        if self.quality_stats:
            logger.info("ðŸ“Š Quality Statistics:")
            for metric, values in self.quality_stats.items():
                if values:
                    logger.info(f"  - {metric}: mean={np.mean(values):.3f}, std={np.std(values):.3f}")

# =============================================================================
# ENHANCED DATASET CLASS
# =============================================================================

class EnhancedDPODataset(Dataset):
    """
    Enhanced DPO dataset with advanced features

    Features:
    - Advanced tokenization and encoding
    - Dynamic batching optimization
    - Quality-based sampling
    - Real-time augmentation
    - Comprehensive caching
    """

    def __init__(self, data_path: str, config: Dict[str, Any], split: str = "train"):
        self.config = config
        self.split = split
        self.max_seq_length = config.get("max_sequence_length", 512)

        # Initialize data processor
        self.processor = AdvancedDPODataProcessor(config)

        # Load and process data
        self.preference_pairs = self.processor.load_and_process_data(data_path)

        # Build vocabulary and tokenizer
        self.vocab = self._build_comprehensive_vocabulary()

        # Create quality-based indices for sampling
        self.quality_indices = self._create_quality_indices()

        # Statistics
        self.stats = self._compute_dataset_statistics()

        logger.info(f"ðŸŽ¯ Enhanced DPO Dataset initialized for {split}")
        logger.info(f"ðŸ“Š Total pairs: {len(self.preference_pairs)}")
        logger.info(f"ðŸ“Š Vocabulary size: {len(self.vocab)}")

    def _build_comprehensive_vocabulary(self) -> Dict[str, int]:
        """Build comprehensive vocabulary from all text sources"""
        word_freq = defaultdict(int)

        # Special tokens
        vocab = {
            "<PAD>": 0, "<UNK>": 1, "<BOS>": 2, "<EOS>": 3,
            "<TOOL>": 4, "<ARG>": 5, "<SEP>": 6, "<MASK>": 7
        }

        # Count words from contexts and trajectories
        for pair in self.preference_pairs:
            # Context words
            context_words = self._tokenize_text(pair.context)
            for word in context_words:
                word_freq[word] += 1

            # Tool names and arguments
            for trajectory in [pair.chosen_trajectory, pair.rejected_trajectory]:
                for step in trajectory.steps:
                    # Tool name
                    word_freq[f"TOOL_{step.tool_name}"] += 1

                    # Arguments
                    for arg_key, arg_value in step.arguments.items():
                        if isinstance(arg_value, str):
                            arg_words = self._tokenize_text(arg_value)
                            for word in arg_words:
                                word_freq[word] += 1

        # Add frequent words to vocabulary
        for word, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True):
            if len(vocab) >= self.config.get("vocab_size", 15000):
                break
            if freq >= self.config.get("min_word_freq", 2):
                vocab[word] = len(vocab)

        logger.info(f"ðŸ”¤ Built vocabulary with {len(vocab)} tokens")
        return vocab

    def _tokenize_text(self, text: str) -> List[str]:
        """Advanced tokenization with preprocessing"""
        # Basic preprocessing
        text = text.lower().strip()

        # Tokenize with regex
        tokens = re.findall(r'\w+|[^\w\s]', text)

        # Filter and clean tokens
        cleaned_tokens = []
        for token in tokens:
            if len(token) > 0 and not token.isspace():
                cleaned_tokens.append(token)

        return cleaned_tokens

    def _create_quality_indices(self) -> Dict[str, List[int]]:
        """Create indices grouped by quality for sampling"""
        quality_groups = {
            "high": [],    # Top 25%
            "medium": [],  # Middle 50%
            "low": []      # Bottom 25%
        }

        # Calculate quality scores
        quality_scores = []
        for i, pair in enumerate(self.preference_pairs):
            # Composite quality score
            score = (
                pair.preference_strength * 0.4 +
                pair.quality_metrics.get("efficiency_advantage", 0) * 0.3 +
                pair.quality_metrics.get("context_relevance", 0) * 0.3
            )
            quality_scores.append((i, score))

        # Sort by quality
        quality_scores.sort(key=lambda x: x[1], reverse=True)

        # Group by quality
        total = len(quality_scores)
        high_cutoff = int(0.25 * total)
        medium_cutoff = int(0.75 * total)

        for i, (idx, score) in enumerate(quality_scores):
            if i < high_cutoff:
                quality_groups["high"].append(idx)
            elif i < medium_cutoff:
                quality_groups["medium"].append(idx)
            else:
                quality_groups["low"].append(idx)

        logger.info(f"ðŸ“Š Quality groups: High={len(quality_groups['high'])}, "
                   f"Medium={len(quality_groups['medium'])}, Low={len(quality_groups['low'])}")

        return quality_groups

    def _compute_dataset_statistics(self) -> Dict[str, Any]:
        """Compute comprehensive dataset statistics"""
        if not self.preference_pairs:
            return {}

        # Basic statistics
        preference_scores = [pair.preference_score for pair in self.preference_pairs]
        preference_strengths = [pair.preference_strength for pair in self.preference_pairs]

        # Trajectory statistics
        chosen_lengths = [pair.chosen_trajectory.total_length for pair in self.preference_pairs]
        rejected_lengths = [pair.rejected_trajectory.total_length for pair in self.preference_pairs]

        # Quality metrics
        efficiency_advantages = [pair.quality_metrics.get("efficiency_advantage", 0)
                               for pair in self.preference_pairs]
        context_relevances = [pair.quality_metrics.get("context_relevance", 0)
                            for pair in self.preference_pairs]

        # Tool usage statistics
        all_chosen_tools = []
        all_rejected_tools = []
        for pair in self.preference_pairs:
            all_chosen_tools.extend([step.tool_name for step in pair.chosen_trajectory.steps])
            all_rejected_tools.extend([step.tool_name for step in pair.rejected_trajectory.steps])

        chosen_tool_dist = Counter(all_chosen_tools)
        rejected_tool_dist = Counter(all_rejected_tools)

        return {
            "total_pairs": len(self.preference_pairs),
            "preference_score_stats": {
                "mean": np.mean(preference_scores),
                "std": np.std(preference_scores),
                "min": np.min(preference_scores),
                "max": np.max(preference_scores)
            },
            "preference_strength_stats": {
                "mean": np.mean(preference_strengths),
                "std": np.std(preference_strengths),
                "strong_preference_ratio": sum(1 for s in preference_strengths if s > 0.7) / len(preference_strengths)
            },
            "trajectory_length_stats": {
                "chosen_mean": np.mean(chosen_lengths),
                "rejected_mean": np.mean(rejected_lengths),
                "chosen_std": np.std(chosen_lengths),
                "rejected_std": np.std(rejected_lengths)
            },
            "quality_metrics": {
                "efficiency_advantage_mean": np.mean(efficiency_advantages),
                "context_relevance_mean": np.mean(context_relevances)
            },
            "tool_usage": {
                "chosen_tools": dict(chosen_tool_dist.most_common(10)),
                "rejected_tools": dict(rejected_tool_dist.most_common(10)),
                "unique_tools": len(set(all_chosen_tools + all_rejected_tools))
            }
        }

    def __len__(self):
        return len(self.preference_pairs)

    def __getitem__(self, idx):
        pair = self.preference_pairs[idx]

        # Tokenize and encode context
        context_tokens = self._tokenize_text(pair.context)
        context_ids = self._tokens_to_ids(context_tokens)

        # Encode trajectories
        chosen_encoding = self._encode_trajectory(pair.chosen_trajectory)
        rejected_encoding = self._encode_trajectory(pair.rejected_trajectory)

        # Pad sequences
        context_ids = self._pad_sequence(context_ids)
        attention_mask = [1 if x != 0 else 0 for x in context_ids]

        return {
            "context": pair.context,
            "context_ids": torch.tensor(context_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "chosen_trajectory": pair.chosen_trajectory,
            "rejected_trajectory": pair.rejected_trajectory,
            "chosen_encoding": chosen_encoding,
            "rejected_encoding": rejected_encoding,
            "preference_score": torch.tensor(pair.preference_score, dtype=torch.float),
            "preference_strength": torch.tensor(pair.preference_strength, dtype=torch.float),
            "quality_metrics": {k: torch.tensor(v, dtype=torch.float)
                              for k, v in pair.quality_metrics.items()},
            "metadata": pair.metadata
        }

    def _tokens_to_ids(self, tokens: List[str]) -> List[int]:
        """Convert tokens to vocabulary IDs"""
        return [self.vocab.get(token, self.vocab["<UNK>"]) for token in tokens]

    def _pad_sequence(self, sequence: List[int]) -> List[int]:
        """Pad sequence to maximum length"""
        if len(sequence) >= self.max_seq_length:
            return sequence[:self.max_seq_length]
        return sequence + [self.vocab["<PAD>"]] * (self.max_seq_length - len(sequence))

    def _encode_trajectory(self, trajectory: ProcessedTrajectory) -> Dict[str, torch.Tensor]:
        """Encode trajectory for model input"""
        # Simple encoding - in practice this would be more sophisticated
        tool_ids = []
        positions = []
        confidences = []

        for step in trajectory.steps:
            tool_name = f"TOOL_{step.tool_name}"
            tool_id = self.vocab.get(tool_name, self.vocab["<UNK>"])
            tool_ids.append(tool_id)
            positions.append(step.position)
            confidences.append(step.confidence)

        # Pad to fixed length
        max_traj_len = 10
        while len(tool_ids) < max_traj_len:
            tool_ids.append(self.vocab["<PAD>"])
            positions.append(0)
            confidences.append(0.0)

        return {
            "tool_ids": torch.tensor(tool_ids[:max_traj_len], dtype=torch.long),
            "positions": torch.tensor(positions[:max_traj_len], dtype=torch.long),
            "confidences": torch.tensor(confidences[:max_traj_len], dtype=torch.float),
            "length": torch.tensor(trajectory.total_length, dtype=torch.long),
            "efficiency": torch.tensor(trajectory.efficiency_score, dtype=torch.float),
            "diversity": torch.tensor(trajectory.tool_diversity, dtype=torch.float)
        }
