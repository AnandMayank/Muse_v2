[
  {
    "model_name": "phi-3-mini",
    "model_type": "SLM",
    "conversation_id": "conv_electronics_search_001",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 0.9835227604868452,
    "ndcg_at_5": 0.9734224497970786,
    "recall_at_1": 0.0,
    "recall_at_3": 0.0,
    "recall_at_5": 0.0,
    "response_coherence": 0.7999999999999999,
    "tool_selection_accuracy": 0.5,
    "conversation_flow_quality": 0.7,
    "user_satisfaction_score": 0.6799999999999999,
    "avg_response_time": 0.7339309453541417,
    "total_tokens_used": 23,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 2000.0
  },
  {
    "model_name": "phi-3-mini",
    "model_type": "SLM",
    "conversation_id": "conv_fashion_multilingual_002",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.991380153914871,
    "recall_at_1": 0.0,
    "recall_at_3": 0.0,
    "recall_at_5": 0.0,
    "response_coherence": 0.7999999999999999,
    "tool_selection_accuracy": 0.1,
    "conversation_flow_quality": 0.7,
    "user_satisfaction_score": 0.5599999999999999,
    "avg_response_time": 0.9699090749728569,
    "total_tokens_used": 18,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 2000.0
  },
  {
    "model_name": "phi-3-mini",
    "model_type": "SLM",
    "conversation_id": "conv_home_decor_simple_003",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9890078334931611,
    "recall_at_1": 0.5,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.8333333333333334,
    "tool_selection_accuracy": 1.0,
    "conversation_flow_quality": 0.7999999999999999,
    "user_satisfaction_score": 0.8733333333333333,
    "avg_response_time": 1.8308556372632434,
    "total_tokens_used": 24,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 2000.0
  },
  {
    "model_name": "phi-3-mini",
    "model_type": "SLM",
    "conversation_id": "conv_sports_comparison_004",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 1.0,
    "recall_at_1": 0.0,
    "recall_at_3": 0.0,
    "recall_at_5": 0.0,
    "response_coherence": 0.7999999999999999,
    "tool_selection_accuracy": 0.5,
    "conversation_flow_quality": 0.7,
    "user_satisfaction_score": 0.6799999999999999,
    "avg_response_time": 0.8126114708982402,
    "total_tokens_used": 22,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 2000.0
  },
  {
    "model_name": "phi-3-mini",
    "model_type": "SLM",
    "conversation_id": "conv_gift_recommendation_005",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9979971989927668,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.7999999999999999,
    "tool_selection_accuracy": 0.6666666666666666,
    "conversation_flow_quality": 0.7,
    "user_satisfaction_score": 0.73,
    "avg_response_time": 0.646353184935712,
    "total_tokens_used": 25,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 2000.0
  },
  {
    "model_name": "gemma-2b",
    "model_type": "SLM",
    "conversation_id": "conv_electronics_search_001",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 0.9874307654170631,
    "ndcg_at_5": 0.9827268446243268,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.8272727272727273,
    "tool_selection_accuracy": 0.5,
    "conversation_flow_quality": 0.8999999999999999,
    "user_satisfaction_score": 0.750909090909091,
    "avg_response_time": 1.687489666512545,
    "total_tokens_used": 26,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 1500.0
  },
  {
    "model_name": "gemma-2b",
    "model_type": "SLM",
    "conversation_id": "conv_fashion_multilingual_002",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9939446745261017,
    "recall_at_1": 0.0,
    "recall_at_3": 0.0,
    "recall_at_5": 0.0,
    "response_coherence": 0.7999999999999999,
    "tool_selection_accuracy": 0.1,
    "conversation_flow_quality": 0.8999999999999999,
    "user_satisfaction_score": 0.6199999999999999,
    "avg_response_time": 0.7818944488364871,
    "total_tokens_used": 21,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 1500.0
  },
  {
    "model_name": "gemma-2b",
    "model_type": "SLM",
    "conversation_id": "conv_home_decor_simple_003",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 0.999506610098443,
    "ndcg_at_5": 0.9996207025666989,
    "recall_at_1": 0.0,
    "recall_at_3": 0.0,
    "recall_at_5": 0.0,
    "response_coherence": 0.8333333333333334,
    "tool_selection_accuracy": 0.1,
    "conversation_flow_quality": 0.9999999999999999,
    "user_satisfaction_score": 0.6633333333333333,
    "avg_response_time": 0.7644262842950722,
    "total_tokens_used": 23,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 1500.0
  },
  {
    "model_name": "gemma-2b",
    "model_type": "SLM",
    "conversation_id": "conv_sports_comparison_004",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9970611392603439,
    "recall_at_1": 0.0,
    "recall_at_3": 0.0,
    "recall_at_5": 0.0,
    "response_coherence": 0.7999999999999999,
    "tool_selection_accuracy": 0.1,
    "conversation_flow_quality": 0.8999999999999999,
    "user_satisfaction_score": 0.6199999999999999,
    "avg_response_time": 1.9868677139360365,
    "total_tokens_used": 23,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 1500.0
  },
  {
    "model_name": "gemma-2b",
    "model_type": "SLM",
    "conversation_id": "conv_gift_recommendation_005",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9921549716183662,
    "recall_at_1": 0.0,
    "recall_at_3": 0.0,
    "recall_at_5": 0.0,
    "response_coherence": 0.7999999999999999,
    "tool_selection_accuracy": 0.1,
    "conversation_flow_quality": 0.8999999999999999,
    "user_satisfaction_score": 0.6199999999999999,
    "avg_response_time": 1.7383417151000775,
    "total_tokens_used": 29,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 1500.0
  },
  {
    "model_name": "llama-3.2-1b",
    "model_type": "SLM",
    "conversation_id": "conv_electronics_search_001",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9970026607558707,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.8272727272727273,
    "tool_selection_accuracy": 0.4,
    "conversation_flow_quality": 0.8999999999999999,
    "user_satisfaction_score": 0.7209090909090909,
    "avg_response_time": 1.7683632831501641,
    "total_tokens_used": 27,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 1500.0
  },
  {
    "model_name": "llama-3.2-1b",
    "model_type": "SLM",
    "conversation_id": "conv_fashion_multilingual_002",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 0.9833606816778403,
    "ndcg_at_5": 0.9802525686249569,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.7999999999999999,
    "tool_selection_accuracy": 0.4,
    "conversation_flow_quality": 0.8999999999999999,
    "user_satisfaction_score": 0.71,
    "avg_response_time": 0.536477200322569,
    "total_tokens_used": 22,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 1500.0
  },
  {
    "model_name": "llama-3.2-1b",
    "model_type": "SLM",
    "conversation_id": "conv_home_decor_simple_003",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9982688192822636,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.8666666666666666,
    "tool_selection_accuracy": 1.0,
    "conversation_flow_quality": 0.9999999999999999,
    "user_satisfaction_score": 0.9466666666666667,
    "avg_response_time": 0.5297599371320589,
    "total_tokens_used": 23,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 1500.0
  },
  {
    "model_name": "llama-3.2-1b",
    "model_type": "SLM",
    "conversation_id": "conv_sports_comparison_004",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9993611237501941,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.83,
    "tool_selection_accuracy": 0.5,
    "conversation_flow_quality": 0.8999999999999999,
    "user_satisfaction_score": 0.752,
    "avg_response_time": 1.5712213409407063,
    "total_tokens_used": 26,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 1500.0
  },
  {
    "model_name": "llama-3.2-1b",
    "model_type": "SLM",
    "conversation_id": "conv_gift_recommendation_005",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9992136640858186,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.84,
    "tool_selection_accuracy": 0.6666666666666666,
    "conversation_flow_quality": 0.8999999999999999,
    "user_satisfaction_score": 0.806,
    "avg_response_time": 1.5844535855184092,
    "total_tokens_used": 30,
    "estimated_cost_usd": 0.0002,
    "memory_usage_mb": 1500.0
  },
  {
    "model_name": "gpt-4",
    "model_type": "LLM",
    "conversation_id": "conv_electronics_search_001",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 0.9984504286949937,
    "ndcg_at_5": 0.9863364035759267,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.7999999999999999,
    "tool_selection_accuracy": 0.8571428571428571,
    "conversation_flow_quality": 0.8999999999999999,
    "user_satisfaction_score": 0.8471428571428572,
    "avg_response_time": 0.5002861022949219,
    "total_tokens_used": 67,
    "estimated_cost_usd": 0.002028,
    "memory_usage_mb": 0.0
  },
  {
    "model_name": "gpt-4",
    "model_type": "LLM",
    "conversation_id": "conv_fashion_multilingual_002",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9881653526527119,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.7999999999999999,
    "tool_selection_accuracy": 0.5714285714285715,
    "conversation_flow_quality": 0.9999999999999999,
    "user_satisfaction_score": 0.7914285714285714,
    "avg_response_time": 0.5002703666687012,
    "total_tokens_used": 62,
    "estimated_cost_usd": 0.001872,
    "memory_usage_mb": 0.0
  },
  {
    "model_name": "gpt-4",
    "model_type": "LLM",
    "conversation_id": "conv_home_decor_simple_003",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 0.9920581351025266,
    "ndcg_at_5": 0.9832926097436787,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.9,
    "tool_selection_accuracy": 0.4,
    "conversation_flow_quality": 0.9999999999999999,
    "user_satisfaction_score": 0.78,
    "avg_response_time": 0.5003054141998291,
    "total_tokens_used": 68,
    "estimated_cost_usd": 0.0020670000000000003,
    "memory_usage_mb": 0.0
  },
  {
    "model_name": "gpt-4",
    "model_type": "LLM",
    "conversation_id": "conv_sports_comparison_004",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 0.9976150309212909,
    "ndcg_at_5": 0.9977714751924514,
    "recall_at_1": 0.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.83,
    "tool_selection_accuracy": 0.8,
    "conversation_flow_quality": 0.9999999999999999,
    "user_satisfaction_score": 0.872,
    "avg_response_time": 0.5002708435058594,
    "total_tokens_used": 68,
    "estimated_cost_usd": 0.0020670000000000003,
    "memory_usage_mb": 0.0
  },
  {
    "model_name": "gpt-4",
    "model_type": "LLM",
    "conversation_id": "conv_gift_recommendation_005",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.984145952260517,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.88,
    "tool_selection_accuracy": 0.6666666666666666,
    "conversation_flow_quality": 0.8999999999999999,
    "user_satisfaction_score": 0.8220000000000001,
    "avg_response_time": 0.5002527236938477,
    "total_tokens_used": 71,
    "estimated_cost_usd": 0.002145,
    "memory_usage_mb": 0.0
  },
  {
    "model_name": "gpt-3.5-turbo",
    "model_type": "LLM",
    "conversation_id": "conv_electronics_search_001",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9995628321995597,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.8272727272727273,
    "tool_selection_accuracy": 0.4,
    "conversation_flow_quality": 0.8999999999999999,
    "user_satisfaction_score": 0.7209090909090909,
    "avg_response_time": 0.5002832412719727,
    "total_tokens_used": 49,
    "estimated_cost_usd": 4.940000000000001e-05,
    "memory_usage_mb": 0.0
  },
  {
    "model_name": "gpt-3.5-turbo",
    "model_type": "LLM",
    "conversation_id": "conv_fashion_multilingual_002",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9971437654001982,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.7999999999999999,
    "tool_selection_accuracy": 0.4,
    "conversation_flow_quality": 0.8999999999999999,
    "user_satisfaction_score": 0.71,
    "avg_response_time": 0.500260591506958,
    "total_tokens_used": 49,
    "estimated_cost_usd": 4.94e-05,
    "memory_usage_mb": 0.0
  },
  {
    "model_name": "gpt-3.5-turbo",
    "model_type": "LLM",
    "conversation_id": "conv_home_decor_simple_003",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9931972969213001,
    "recall_at_1": 0.0,
    "recall_at_3": 0.0,
    "recall_at_5": 0.0,
    "response_coherence": 0.7999999999999999,
    "tool_selection_accuracy": 0.1,
    "conversation_flow_quality": 0.9999999999999999,
    "user_satisfaction_score": 0.6499999999999999,
    "avg_response_time": 0.5002765655517578,
    "total_tokens_used": 45,
    "estimated_cost_usd": 4.550000000000001e-05,
    "memory_usage_mb": 0.0
  },
  {
    "model_name": "gpt-3.5-turbo",
    "model_type": "LLM",
    "conversation_id": "conv_sports_comparison_004",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 1.0,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.83,
    "tool_selection_accuracy": 0.5,
    "conversation_flow_quality": 0.9999999999999999,
    "user_satisfaction_score": 0.7819999999999999,
    "avg_response_time": 0.5003058910369873,
    "total_tokens_used": 53,
    "estimated_cost_usd": 5.330000000000001e-05,
    "memory_usage_mb": 0.0
  },
  {
    "model_name": "gpt-3.5-turbo",
    "model_type": "LLM",
    "conversation_id": "conv_gift_recommendation_005",
    "ndcg_at_1": 1.0,
    "ndcg_at_3": 1.0,
    "ndcg_at_5": 0.9849404007756538,
    "recall_at_1": 1.0,
    "recall_at_3": 1.0,
    "recall_at_5": 1.0,
    "response_coherence": 0.86,
    "tool_selection_accuracy": 1.0,
    "conversation_flow_quality": 0.8999999999999999,
    "user_satisfaction_score": 0.9139999999999999,
    "avg_response_time": 0.5003101825714111,
    "total_tokens_used": 57,
    "estimated_cost_usd": 5.72e-05,
    "memory_usage_mb": 0.0
  }
]